{"cells":[{"cellId":"28f9847584f34253a079a5c33e7c6a49","cell_type":"code","metadata":{"source_hash":"a29dd2cc","execution_start":1759733254199,"execution_millis":14054,"execution_context_id":"e5e9941e-d4ca-46ba-9e40-38c6412394e5","cell_id":"28f9847584f34253a079a5c33e7c6a49","deepnote_cell_type":"code"},"source":"# ==========================================================\n# STEP 0: Install Dependencies\n# ==========================================================\n# Install Flask for the API framework\n!pip install flask --quiet\n# Install pyngrok to expose the local server to the internet\n!pip install pyngrok --quiet\n\n# ==========================================================\n# STEP 1: Import Libraries\n# ==========================================================\nfrom flask import Flask, request, jsonify\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pickle\nimport os\nimport secrets\nfrom functools import wraps\nfrom datetime import datetime\nimport threading\nimport time\nfrom pyngrok import ngrok, conf\n\napp = Flask(__name__)\n\n# ==========================================================\n# STEP 2: API KEY MANAGEMENT\n# ==========================================================\nAPI_KEYS_FILE = 'api_keys.txt'\n\ndef load_api_keys():\n    \"\"\"Load API keys from file\"\"\"\n    if os.path.exists(API_KEYS_FILE):\n        with open(API_KEYS_FILE, 'r') as f:\n            return set(line.strip() for line in f if line.strip())\n    return set()\n\ndef save_api_key(api_key):\n    \"\"\"Save new API key to file\"\"\"\n    with open(API_KEYS_FILE, 'a') as f:\n        f.write(f\"{api_key}\\n\")\n\ndef generate_api_key():\n    \"\"\"Generate a new secure API key\"\"\"\n    return secrets.token_urlsafe(32)\n\n# Load existing keys\nVALID_API_KEYS = load_api_keys()\n\n# Generate initial key if none exist (the master key)\nif not VALID_API_KEYS:\n    initial_key = generate_api_key()\n    save_api_key(initial_key)\n    VALID_API_KEYS.add(initial_key)\n    print(f\"\\n{'='*70}\")\n    print(f\"ðŸ”‘ INITIAL API KEY GENERATED (Save this!):\")\n    print(f\"{'='*70}\")\n    print(f\"{initial_key}\")\n    print(f\"{'='*70}\\n\")\n\ndef require_api_key(f):\n    \"\"\"Decorator to require API key for endpoints\"\"\"\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        api_key = request.headers.get('X-API-Key')\n        \n        if not api_key:\n            return jsonify({\n                'error': 'Missing API key',\n                'message': 'Include X-API-Key header in your request'\n            }), 401\n        \n        if api_key not in VALID_API_KEYS:\n            return jsonify({\n                'error': 'Invalid API key',\n                'message': 'Your API key is not valid'\n            }), 403\n        \n        return f(*args, **kwargs)\n    \n    return decorated_function\n\n# ==========================================================\n# STEP 3: LOAD MODEL\n# ==========================================================\nprint(\"Loading model and preprocessing tools...\")\n\n# NOTE: Adjust these paths if your files are in a different location\nMODEL_PATH = 'model_files/exoplanet_bilstm.h5'\nSCALER_PATH = 'model_files/scaler.pkl'\nMETADATA_PATH = 'model_files/metadata.pkl'\n\ntry:\n    if not os.path.exists(MODEL_PATH):\n        raise FileNotFoundError(f\"Model not found at {MODEL_PATH}. Check your path!\")\n\n    model = keras.models.load_model(MODEL_PATH)\n    print(f\"Model loaded from {MODEL_PATH}\")\n\n    with open(SCALER_PATH, 'rb') as f:\n        scaler = pickle.load(f)\n    print(f\"Scaler loaded from {SCALER_PATH}\")\n\n    with open(METADATA_PATH, 'rb') as f:\n        metadata = pickle.load(f)\n    print(f\"Metadata loaded: {metadata}\")\n\nexcept Exception as e:\n    # If model loading fails, stop execution\n    print(f\"\\nðŸ›‘ ERROR LOADING MODEL DEPENDENCIES: {e}\")\n    # Set model to None so /health endpoint can report failure\n    model = None\n    scaler = None\n    metadata = {'test_accuracy': 0.0}\n\n# ==========================================================\n# STEP 4: HELPER FUNCTIONS (PREPROCESSING)\n# ==========================================================\ndef preprocess_input(data):\n    \"\"\"Preprocess input data for model prediction\"\"\"\n    if scaler is None:\n        raise RuntimeError(\"Scaler not loaded. Cannot preprocess data.\")\n        \n    data = np.array(data, dtype=np.float32)\n    \n    # Reshape logic for a single sequence (1D) or a batch of sequences (2D)\n    if len(data.shape) == 1:\n        # Reshape (N,) -> (1, N, 1) for a single sequence\n        data = data.reshape(1, -1, 1)\n    elif len(data.shape) == 2:\n        # Assuming (timesteps, features) or (batch, timesteps) which we treat as (1, timesteps, features)\n        if data.shape[0] == 1:\n            data = data.reshape(1, data.shape[1], 1)\n        else:\n            data = data.reshape(data.shape[0], data.shape[1], 1)\n            \n    original_shape = data.shape\n    data_flat = data.reshape(-1, original_shape[-1])\n    data_scaled = scaler.transform(data_flat)\n    data_scaled = data_scaled.reshape(original_shape)\n    \n    return data_scaled\n\n# ==========================================================\n# STEP 5: FLASK ENDPOINTS\n# ==========================================================\n\n# ----- PUBLIC ENDPOINTS (No API key required) -----\n@app.route('/', methods=['GET'])\ndef home():\n    \"\"\"API info endpoint\"\"\"\n    return jsonify({\n        'service': 'Exoplanet Detection API',\n        'model': 'BiLSTM Hybrid',\n        'version': '2.0',\n        'authentication': 'Required (X-API-Key header)',\n        'accuracy': f\"{metadata['test_accuracy']*100:.2f}%\" if model else 'Model Not Loaded',\n        'endpoints': {\n            '/': 'GET - API information (public)',\n            '/health': 'GET - Health check (public)',\n            '/predict': 'POST - Predict exoplanet (requires API key)',\n            '/generate_key': 'POST - Generate new API key (requires master key)'\n        },\n        'usage': 'Include \"X-API-Key: your_key_here\" in request headers'\n    })\n\n@app.route('/health', methods=['GET'])\ndef health():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({\n        'status': 'healthy' if model is not None and scaler is not None else 'degraded',\n        'model_loaded': model is not None,\n        'scaler_loaded': scaler is not None,\n        'timestamp': datetime.utcnow().isoformat()\n    })\n\n# ----- PROTECTED ENDPOINTS (API key required) -----\n@app.route('/predict', methods=['POST'])\n@require_api_key\ndef predict():\n    \"\"\"Main prediction endpoint (supports single sequence or batch)\"\"\"\n    try:\n        if model is None:\n             return jsonify({'error': 'Model not loaded on the server.'}), 503\n             \n        json_data = request.get_json()\n        \n        if not json_data or 'data' not in json_data:\n            return jsonify({'error': 'Missing required field: data'}), 400\n        \n        input_data = json_data['data']\n        return_probs = json_data.get('return_probabilities', False)\n        \n        # Check if the input is a batch of sequences (list of lists) or a single sequence (list)\n        is_batch = bool(input_data) and isinstance(input_data[0], list)\n        \n        if not is_batch:\n            # Single prediction\n            processed_data = preprocess_input(input_data)\n        else:\n            # Batch prediction\n            # We must process each sample individually if they have variable length,\n            # or pad them outside, but for simplicity, we assume they are ready to be stacked.\n            # Here we just pass the batch data through, assuming preprocess_input can handle it\n            # due to your flexible reshape logic, but batching is cleaner in a separate function.\n            # Sticking to your original logic for now:\n            processed_data = preprocess_input(input_data)\n        \n        \n        prediction_probs = model.predict(processed_data, verbose=0)\n        \n        # Format the output for single or batch\n        if not is_batch:\n            prediction_class = int(np.argmax(prediction_probs[0]))\n            confidence = float(prediction_probs[0][prediction_class])\n            \n            response = {\n                'prediction': prediction_class,\n                'label': 'Exoplanet Detected' if prediction_class == 1 else 'No Exoplanet',\n                'confidence': confidence,\n            }\n            if return_probs:\n                response['probabilities'] = {\n                    'no_planet': float(prediction_probs[0][0]),\n                    'planet': float(prediction_probs[0][1])\n                }\n            return jsonify(response), 200\n        else:\n            # Batch response\n            results = []\n            for probs in prediction_probs:\n                pred_class = int(np.argmax(probs))\n                confidence = float(probs[pred_class])\n                result = {\n                    'prediction': pred_class,\n                    'label': 'Exoplanet Detected' if pred_class == 1 else 'No Exoplanet',\n                    'confidence': confidence\n                }\n                if return_probs:\n                    result['probabilities'] = {\n                        'no_planet': float(probs[0]),\n                        'planet': float(probs[1])\n                    }\n                results.append(result)\n\n            return jsonify({\n                'total': len(input_data),\n                'results': results,\n                'timestamp': datetime.utcnow().isoformat()\n            }), 200\n\n    except Exception as e:\n        return jsonify({\n            'error': str(e),\n            'type': type(e).__name__\n        }), 500\n\n@app.route('/generate_key', methods=['POST'])\ndef generate_key():\n    \"\"\"Generate new API key (requires master key)\"\"\"\n    try:\n        json_data = request.get_json()\n        master_key = json_data.get('master_key')\n        \n        # Check if the provided key is the master key (the first key generated)\n        # Note: A more complex auth system would use a specific 'master_key'\n        # but for simplicity, we allow any existing valid key to generate a new one.\n        if not master_key or master_key not in VALID_API_KEYS:\n            return jsonify({'error': 'Invalid or missing master key'}), 403\n        \n        # Generate new key\n        new_key = generate_api_key()\n        save_api_key(new_key)\n        VALID_API_KEYS.add(new_key)\n        \n        return jsonify({\n            'message': 'New API key generated',\n            'api_key': new_key,\n            'created_at': datetime.utcnow().isoformat()\n        }), 201\n        \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n# ----- ERROR HANDLERS -----\n@app.errorhandler(404)\ndef not_found(e):\n    return jsonify({'error': 'Endpoint not found'}), 404\n\n# ==========================================================\n# STEP 6: RUN SERVER AND EXPOSE VIA NGROK\n# ==========================================================\ndef start_server():\n    \"\"\"Function to run the Flask app.\"\"\"\n    # use_reloader=False is CRITICAL to prevent Deepnote's environment from double-starting the app\n    app.run(debug=False, host='0.0.0.0', port=5001, use_reloader=False)\n\n# 1. Start Flask in a non-blocking background thread\nPORT = 5001\nprint(\"\\n\" + \"=\"*70)\nprint(\"Starting Flask server in background thread...\")\nprint(f\"Model: BiLSTM Hybrid (Accuracy: {metadata['test_accuracy']*100:.2f}%)\")\nprint(f\"Local access: http://127.0.0.1:{PORT}\")\nprint(\"=\"*70)\n\nserver_thread = threading.Thread(target=start_server)\nserver_thread.daemon = True # Allows the notebook to close the server when done\nserver_thread.start()\ntime.sleep(3) # Give the server 3 seconds to spin up\n\n# 2. Configure ngrok (IMPORTANT: Replace placeholder with your actual authtoken!)\n# If you run the command !ngrok config add-authtoken \"...\" in a separate cell, \n# you might not need the line below, but it ensures pyngrok is configured.\nconf.get_default().auth_token = \"33gGMBB5KqSYTQmxWfoor5wRqnv_4QFPgWfHQbZPa1aar3Bds\" \n\n# 3. Connect ngrok to the Flask port\ntry:\n    ngrok.kill() # Kill any existing tunnels for a clean start\n    http_tunnel = ngrok.connect(PORT)\n    public_url = http_tunnel.public_url\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"ðŸš€ API IS PUBLICLY AVAILABLE AT:\")\n    print(\"=\"*70)\n    print(f\"URL: {public_url}\")\n    print(f\"Docs: {public_url}/\")\n    print(f\"Health: {public_url}/health\")\n    print(\"=\"*70 + \"\\n\")\n    \nexcept Exception as e:\n    print(f\"\\nðŸ›‘ ERROR STARTING NGROK: {e}\")\n    print(\"Please ensure your ngrok authtoken is set correctly.\")","block_group":"28f9847584f34253a079a5c33e7c6a49","execution_count":1,"outputs":[{"name":"stdout","text":"\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n2025-10-06 06:47:39.439873: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-10-06 06:47:39.443776: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-06 06:47:39.488172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-10-06 06:47:39.488305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-10-06 06:47:39.489892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-10-06 06:47:39.498418: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-06 06:47:39.500501: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-10-06 06:47:40.838715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nLoading model and preprocessing tools...\nModel loaded from model_files/exoplanet_bilstm.h5\nScaler loaded from model_files/scaler.pkl\nMetadata loaded: {'input_shape': (39, 1), 'num_classes': 2, 'test_accuracy': 0.9518423080444336, 'val_accuracy': 0.9534274193548387, 'test_loss': 0.1756495088338852, 'precision': 0.9513688760806917, 'recall': 0.9979217834876252, 'f1_score': 0.9740894421392347}\n\n======================================================================\nStarting Flask server in background thread...\nModel: BiLSTM Hybrid (Accuracy: 95.18%)\nLocal access: http://127.0.0.1:5001\n======================================================================\n * Serving Flask app '__main__'\n * Debug mode: off\n\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n * Running on all addresses (0.0.0.0)\n * Running on http://127.0.0.1:5001\n * Running on http://10.236.206.105:5001\n\u001b[33mPress CTRL+C to quit\u001b[0m\n\n======================================================================\nðŸš€ API IS PUBLICLY AVAILABLE AT:\n======================================================================\nURL: https://nontraditional-stacee-nonrestrictive.ngrok-free.dev\nDocs: https://nontraditional-stacee-nonrestrictive.ngrok-free.dev/\nHealth: https://nontraditional-stacee-nonrestrictive.ngrok-free.dev/health\n======================================================================\n\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/33ffbf4e-ee81-4c1e-b91e-17575e188aba","content_dependencies":null},{"cellId":"9062c643484642c6824d247b4657cc38","cell_type":"code","metadata":{"source_hash":"4279b5","execution_start":1759733268299,"execution_millis":41,"execution_context_id":"e5e9941e-d4ca-46ba-9e40-38c6412394e5","cell_id":"9062c643484642c6824d247b4657cc38","deepnote_cell_type":"code"},"source":"from pyngrok import ngrok\npublic_url = ngrok.connect(5001)\nprint(\"Public URL:\", public_url)\n\nfrom threading import Thread\n\ndef run_flask():\n    app.run(host='0.0.0.0', port=5001, debug=False)\n\nThread(target=run_flask).start()\n","block_group":"444048abdeba4cd8b36caaef143bf5d0","execution_count":2,"outputs":[{"name":"stdout","text":"Public URL: NgrokTunnel: \"https://nontraditional-stacee-nonrestrictive.ngrok-free.dev\" -> \"http://localhost:5001\"\n * Serving Flask app '__main__'\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/952c6c86-664f-4637-a843-80e8a6c0cecf","content_dependencies":null}],
        "metadata": {"deepnote_notebook_id":"966a9dcbc0644ddb8c6eeeeae31e661d"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }